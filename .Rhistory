p<-ggplot(subset(wf,freqr>100),aes(term,occurrences))
p<-p+geom_bar(stat="identity")
p<-p+theme(axis.text.x=element_text(angle=45,hjust=1))
p
p<-ggplot(subset(wf,freqr>10),aes(term,occurrences))
p<-p+geom_bar(stat="identity")
p<-p+theme(axis.text.x=element_text(angle=45,hjust=1))
p
p<-ggplot(subset(wf,freqr>20),aes(term,occurrences))
p<-p+geom_bar(stat="identity")
p<-p+theme(axis.text.x=element_text(angle=45,hjust=1))
p
library(ggplot2)
p<-ggplot(subset(wf,freqr>50),aes(term,occurrences))
p<-p+geom_bar(stat="identity")
p<-p+theme(axis.text.x=element_text(angle=45,hjust=1))
p
p<-ggplot(subset(wf,freqr>25),aes(term,occurrences))
p<-p+geom_bar(stat="identity")
p<-p+theme(axis.text.x=element_text(angle=45,hjust=1))
p
install.packages("wordcloud")
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
#setting the same seeds for consistent look
set.seed(30)
#limit words by specifying the min frequency
wordcloud(names(freqr),freqr,min.freq=25)
#..now adding color
wordcloud(names(freqr),freqr,min.freq = 25,colors=brewer.pal(6,"Dark2"))
#setting the same seeds for consistent look
set.seed(30)
#limit words by specifying the min frequency
wordcloud(names(freqr),freqr,min.freq=25)
#..now adding color
wordcloud(names(freqr),freqr,min.freq = 25,colors=brewer.pal(6,"Dark2"))
wordcloud(names(freqr),freqr,min.freq = 27,colors=brewer.pal(6,"Dark2"))
wordcloud(names(freqr),freqr,min.freq = 24,colors=brewer.pal(6,"Dark2"))
#installing tm Packages
install.packages("tm")
library(tm)
#Loading required package: NLP
#create Corpus
#loading a text file from local computer
newdata<- readLines("/Users/thema/Downloads/The Sherlock Holmes.txt")
#Load data as corpus
#VectorSource() creates character vectors
docs <- Corpus(VectorSource(newdata))
inspect(docs)
writeLines(as.character(docs[[30]]))
#start preprocessing
toSpace <- content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
docs<-tm_map(docs, toSpace,"_")
docs<-tm_map(docs,toSpace,":")
docs<-tm_map(docs,toSpace,";")
docs<-tm_map(docs,toSpace,"'")
docs<-tm_map(docs,toSpace,"-")
docs<-tm_map(docs,toSpace," '' ")
#Remove Punctuations
docs <- tm_map(docs,removePunctuation)
#transform to lower case
docs<-tm_map(docs,content_transformer(tolower))
#strip digits
docs<-tm_map(docs,removeNumbers)
#Remove stopwords from standard stopwords list
docs <-tm_map(docs,removeWords,stopwords("english"))
#strip whitespaces
docs<-tm_map(docs, stripWhitespace)
#inspect Output
inspect(docs)
#need snowballC library for stemming
install.packages("SnowballC")
library(SnowballC)
#stem document
docs<- tm_map(docs, stemDocument)
#some Clean Up
docs<-tm_map(docs, content_transformer(gsub), pattern="oraganiz",replacement="organ")
docs<-tm_map(docs, content_transformer(gsub), pattern="oraganis",replacement="organ")
docs<-tm_map(docs, content_transformer(gsub), pattern="andovern",replacement="govern")
docs<-tm_map(docs, content_transformer(gsub), pattern="inenteriors",replacement="enterprize")
docs<-tm_map(docs, content_transformer(gsub), pattern="team-",replacement="team")
#inspect output
inspect(docs)
#create a document matrix
dtm<-DocumentTermMatrix(docs)
#inspect
inspect(dtm[1:6,1001:1007])
#collapse matrix by summing over columns- this gets total counts (over all docs )for each term
freq <-colSums(as.matrix(dtm))
#length should be total numbers of terms
length(freq)
#creating a sorting order
ord<-order(freq,decreasing = TRUE)
#inspect most frequently occurring terms
freq[head(ord)]
#inspect least frequently occurring terms
freq[tail(ord)]
#remove very frequent and very rare words
dtmr<-DocumentTermMatrix(docs, control = list(wordLengths=c(4,20),bounds=list(global=c(3,27))))
freq
#installing tm Packages
install.packages("tm")
library(tm)
install.packages("tm")
#create Corpus
#loading a text file from local computer
newdata<- readLines("/Users/thema/Downloads/The Sherlock Holmes.txt")
#Load data as corpus
#VectorSource() creates character vectors
docs <- Corpus(VectorSource(newdata))
install.packages("tm")
inspect(docs)
install.packages("tm")
install.packages("tm")
install.packages("tm")
install.packages("tm")
writeLines(as.character(docs[[30]]))
#start preprocessing
toSpace <- content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
docs<-tm_map(docs, toSpace,"_")
docs<-tm_map(docs,toSpace,":")
docs<-tm_map(docs,toSpace,";")
docs<-tm_map(docs,toSpace,"'")
docs<-tm_map(docs,toSpace,"-")
docs<-tm_map(docs,toSpace," '' ")
#Remove Punctuations
docs <- tm_map(docs,removePunctuation)
#transform to lower case
docs<-tm_map(docs,content_transformer(tolower))
#strip digits
docs<-tm_map(docs,removeNumbers)
#Remove stopwords from standard stopwords list
docs <-tm_map(docs,removeWords,stopwords("english"))
#strip whitespaces
docs<-tm_map(docs, stripWhitespace)
#inspect Output
inspect(docs)
#need snowballC library for stemming
install.packages("SnowballC")
library(SnowballC)
docs<- tm_map(docs, stemDocument)
docs<-tm_map(docs, content_transformer(gsub), pattern="andovern",replacement="govern")
docs<-tm_map(docs, content_transformer(gsub), pattern="inenteriors",replacement="enterprize")
docs<-tm_map(docs, content_transformer(gsub), pattern="team-",replacement="team")
#inspect output
inspect(docs)
#create a document matrix
dtm<-DocumentTermMatrix(docs)
#inspect
inspect(dtm[1:6,1001:1007])
#collapse matrix by summing over columns- this gets total counts (over all docs )for each term
freq <-colSums(as.matrix(dtm))
#length should be total numbers of terms
length(freq)
#creating a sorting order
ord<-order(freq,decreasing = TRUE)
#inspect most frequently occurring terms
freq[head(ord)]
#inspect least frequently occurring terms
freq[tail(ord)]
dtmr<-DocumentTermMatrix(docs, control = list(wordLengths=c(4,20),bounds=list(global=c(3,27))))
freqr<-colSums(as.matrix(dtmr))
#length should be total number of terms
length(freqr)
#now creating a sorting order
ordr<-order(freqr,decreasing = TRUE)
#inspect most frequently occurring terms
freqr[head(ordr)]
#inspect most least occurring terms
freqr[tail(ordr)]
#listing most frequents terms , lower bound specified as second argument
findFreqTerms(dtmr,lowfreq = 10)
#correlations
findAssocs(dtmr,"throw",0.1)
findAssocs(dtmr,"form",0.1)
findAssocs(dtmr,"taken",0.1)
findAssocs(dtmr,"spoke",0.1)
#histogram
wf=data.frame(term=names(freqr),occurrences=freqr)
install.packages("ggplot2")
install.packages("ggplot2")
library(ggplot2)
p<-ggplot(subset(wf,freqr>25),aes(term,occurrences))
p<-p+geom_bar(stat="identity")
p<-p+theme(axis.text.x=element_text(angle=45,hjust=1))
p
#wordcloud
install.packages("wordcloud")
library(wordcloud)
#setting the same seeds for consistent look
set.seed(30)
#inspect most frequently occurring terms
freqr[head(ordr)]
#inspect most least occurring terms
freqr[tail(ordr)]
#listing most frequents terms , lower bound specified as second argument
findFreqTerms(dtmr,lowfreq = 10)
#correlations
findAssocs(dtmr,"throw",0.1)
findAssocs(dtmr,"form",0.1)
findAssocs(dtmr,"taken",0.1)
findAssocs(dtmr,"spoke",0.1)
#histogram
wf=data.frame(term=names(freqr),occurrences=freqr)
install.packages("ggplot2")
install.packages("ggplot2")
#installing tm Packages
install.packages("tm")
install.packages("tm")
install.packages("tm")
install.packages("tm")
install.packages("wordcloud")
install.packages("ggplot2")
install.packages("ggplot2")
library(tm)
#create Corpus
#loading a text file from local computer
newdata<- readLines("/Users/thema/Downloads/The Sherlock Holmes.txt")
#Load data as corpus
#VectorSource() creates character vectors
docs <- Corpus(VectorSource(newdata))
inspect(docs)
writeLines(as.character(docs[[30]]))
#start preprocessing
toSpace <- content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
docs<-tm_map(docs, toSpace,"_")
docs<-tm_map(docs,toSpace,":")
docs<-tm_map(docs,toSpace,";")
docs<-tm_map(docs,toSpace,"'")
docs<-tm_map(docs,toSpace,"-")
docs<-tm_map(docs,toSpace," '' ")
#Remove Punctuations
docs <- tm_map(docs,removePunctuation)
#transform to lower case
docs<-tm_map(docs,content_transformer(tolower))
#strip digits
docs<-tm_map(docs,removeNumbers)
#Remove stopwords from standard stopwords list
docs <-tm_map(docs,removeWords,stopwords("english"))
#strip whitespaces
docs<-tm_map(docs, stripWhitespace)
#inspect Output
inspect(docs)
#need snowballC library for stemming
install.packages("SnowballC")
library(SnowballC)
docs<- tm_map(docs, stemDocument)
#some Clean Up
docs<-tm_map(docs, content_transformer(gsub), pattern="oraganiz",replacement="organ")
docs<-tm_map(docs, content_transformer(gsub), pattern="oraganis",replacement="organ")
docs<-tm_map(docs, content_transformer(gsub), pattern="andovern",replacement="govern")
docs<-tm_map(docs, content_transformer(gsub), pattern="inenteriors",replacement="enterprize")
docs<-tm_map(docs, content_transformer(gsub), pattern="team-",replacement="team")
#inspect output
inspect(docs)
#create a document matrix
dtm<-DocumentTermMatrix(docs)
#inspect
inspect(dtm[1:6,1001:1007])
#collapse matrix by summing over columns- this gets total counts (over all docs )for each term
freq <-colSums(as.matrix(dtm))
#length should be total numbers of terms
length(freq)
#creating a sorting order
ord<-order(freq,decreasing = TRUE)
#inspect most frequently occurring terms
freq[head(ord)]
#inspect least frequently occurring terms
freq[tail(ord)]
dtmr<-DocumentTermMatrix(docs, control = list(wordLengths=c(4,20),bounds=list(global=c(3,27))))
freqr<-colSums(as.matrix(dtmr))
#length should be total number of terms
length(freqr)
#now creating a sorting order
ordr<-order(freqr,decreasing = TRUE)
#inspect most frequently occurring terms
freqr[head(ordr)]
#inspect most least occurring terms
freqr[tail(ordr)]
#listing most frequents terms , lower bound specified as second argument
findFreqTerms(dtmr,lowfreq = 10)
#listing most frequents terms , lower bound specified as second argument
findFreqTerms(dtmr,lowfreq = 15)
#correlations
findAssocs(dtmr,"throw",0.1)
findAssocs(dtmr,"form",0.1)
findAssocs(dtmr,"taken",0.1)
findAssocs(dtmr,"spoke",0.1)
#histogram
wf=data.frame(term=names(freqr),occurrences=freqr)
library(ggplot2)
p<-ggplot(subset(wf,freqr>25),aes(term,occurrences))
p<-p+geom_bar(stat="identity")
p<-p+theme(axis.text.x=element_text(angle=45,hjust=1))
p
#limit words by specifying the min frequency
wordcloud(names(freqr),freqr,min.freq=20)
#..now adding color
wordcloud(names(freqr),freqr,min.freq = 25,colors=brewer.pal(6,"Dark2"))
library(ggplot2)
p<-ggplot(subset(wf,freqr>20),aes(term,occurrences))
p<-p+geom_bar(stat="identity")
p<-p+theme(axis.text.x=element_text(angle=45,hjust=1))
p
p<-ggplot(subset(wf,freqr>27),aes(term,occurrences))
p<-p+geom_bar(stat="identity")
p<-p+theme(axis.text.x=element_text(angle=45,hjust=1))
p
p<-ggplot(subset(wf,freqr>26),aes(term,occurrences))
p<-p+geom_bar(stat="identity")
p<-p+theme(axis.text.x=element_text(angle=45,hjust=1))
p
library(wordcloud)
#setting the same seeds for consistent look
set.seed(30)
#limit words by specifying the min frequency
wordcloud(names(freqr),freqr,min.freq=20)
#..now adding color
wordcloud(names(freqr),freqr,min.freq = 25,colors=brewer.pal(6,"Dark2"))
l
l
l
library(wordcloud)
#setting the same seeds for consistent look
set.seed(30)
#limit words by specifying the min frequency
wordcloud(names(freqr),freqr,min.freq=26)
#..now adding color
wordcloud(names(freqr),freqr,min.freq = 25,colors=brewer.pal(6,"Dark2"))
library(wordcloud)
#setting the same seeds for consistent look
set.seed(30)
#limit words by specifying the min frequency
wordcloud(names(freqr),freqr,min.freq=26)
#..now adding color
wordcloud(names(freqr),freqr,min.freq = 25,colors=brewer.pal(6,"Dark2"))
#inspect most least occurring terms
freqr[tail(ordr)]
#listing most frequents terms , lower bound specified as second argument
findFreqTerms(dtmr,lowfreq = 15)
#correlations
findAssocs(dtmr,"throw",0.1)
findAssocs(dtmr,"form",0.1)
findAssocs(dtmr,"taken",0.1)
findAssocs(dtmr,"spoke",0.1)
#histogram
wf=data.frame(term=names(freqr),occurrences=freqr)
library(ggplot2)
p<-ggplot(subset(wf,freqr>26),aes(term,occurrences))
p<-p+geom_bar(stat="identity")
p<-p+theme(axis.text.x=element_text(angle=45,hjust=1))
p
library(wordcloud)
#setting the same seeds for consistent look
set.seed(30)
#limit words by specifying the min frequency
wordcloud(names(freqr),freqr,min.freq=27)
#..now adding color
wordcloud(names(freqr),freqr,min.freq = 25,colors=brewer.pal(6,"Dark2"))
#limit words by specifying the min frequency
wordcloud(names(freqr),freqr,min.freq=27)
#..now adding color
wordcloud(names(freqr),freqr,min.freq = 27,colors=brewer.pal(6,"Dark2"))
#create Corpus
#loading a text file from local computer
newdata<- readLines("https://raw.githubusercontent.com/dgrtwo/tidy-text-mining/master/data/tweets_julia.csv")
#Load data as corpus
#VectorSource() creates character vectors
docs <- Corpus(VectorSource(newdata))
inspect(docs)
writeLines(as.character(docs[[30]]))
#start preprocessing
toSpace <- content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
docs<-tm_map(docs, toSpace,"_")
docs<-tm_map(docs,toSpace,":")
docs<-tm_map(docs,toSpace,";")
docs<-tm_map(docs,toSpace,"'")
docs<-tm_map(docs,toSpace,"-")
docs<-tm_map(docs,toSpace," '' ")
#Remove Punctuations
docs <- tm_map(docs,removePunctuation)
#transform to lower case
docs<-tm_map(docs,content_transformer(tolower))
#strip digits
docs<-tm_map(docs,removeNumbers)
#Remove stopwords from standard stopwords list
docs <-tm_map(docs,removeWords,stopwords("english"))
#strip whitespaces
docs<-tm_map(docs, stripWhitespace)
#inspect Output
inspect(docs)
#need snowballC library for stemming
install.packages("SnowballC")
install.packages("SnowballC")
#installing tm Packages
install.packages("tm")
install.packages("tm")
install.packages("SnowballC")
install.packages("SnowballC")
install.packages("wordcloud")
install.packages("wordcloud")
install.packages("ggplot2")
install.packages("ggplot2")
library(tm)
#create Corpus
#loading a text file from local computer
newdata<- readLines("https://raw.githubusercontent.com/dgrtwo/tidy-text-mining/master/data/tweets_julia.csv")
#Load data as corpus
#VectorSource() creates character vectors
docs <- Corpus(VectorSource(newdata))
inspect(docs)
writeLines(as.character(docs[[30]]))
#start preprocessing
toSpace <- content_transformer(function(x,pattern){return(gsub(pattern," ",x))})
docs<-tm_map(docs, toSpace,"_")
docs<-tm_map(docs,toSpace,":")
docs<-tm_map(docs,toSpace,";")
docs<-tm_map(docs,toSpace,"'")
docs<-tm_map(docs,toSpace,"-")
docs<-tm_map(docs,toSpace," '' ")
#Remove Punctuations
docs <- tm_map(docs,removePunctuation)
#transform to lower case
docs<-tm_map(docs,content_transformer(tolower))
#strip digits
docs<-tm_map(docs,removeNumbers)
#Remove stopwords from standard stopwords list
docs <-tm_map(docs,removeWords,stopwords("english"))
#strip whitespaces
docs<-tm_map(docs, stripWhitespace)
#inspect Output
inspect(docs)
library(SnowballC)
docs<- tm_map(docs, stemDocument)
#some Clean Up
docs<-tm_map(docs, content_transformer(gsub), pattern="oraganiz",replacement="organ")
docs<-tm_map(docs, content_transformer(gsub), pattern="oraganis",replacement="organ")
docs<-tm_map(docs, content_transformer(gsub), pattern="andovern",replacement="govern")
docs<-tm_map(docs, content_transformer(gsub), pattern="inenteriors",replacement="enterprize")
docs<-tm_map(docs, content_transformer(gsub), pattern="team-",replacement="team")
#inspect output
inspect(docs)
#create a document matrix
dtm<-DocumentTermMatrix(docs)
#inspect
inspect(dtm[1:6,1001:1007])
#collapse matrix by summing over columns- this gets total counts (over all docs )for each term
freq <-colSums(as.matrix(dtm))
#length should be total numbers of terms
length(freq)
#creating a sorting order
ord<-order(freq,decreasing = TRUE)
#inspect most frequently occurring terms
freq[head(ord)]
#inspect least frequently occurring terms
freq[tail(ord)]
dtmr<-DocumentTermMatrix(docs, control = list(wordLengths=c(4,20),bounds=list(global=c(3,27))))
freqr<-colSums(as.matrix(dtmr))
#length should be total number of terms
length(freqr)
#now creating a sorting order
ordr<-order(freqr,decreasing = TRUE)
#inspect most frequently occurring terms
freqr[head(ordr)]
#inspect most least occurring terms
freqr[tail(ordr)]
#listing most frequents terms , lower bound specified as second argument
findFreqTerms(dtmr,lowfreq = 15)
#correlations
findAssocs(dtmr,"throw",0.1)
findAssocs(dtmr,"form",0.1)
findAssocs(dtmr,"taken",0.1)
findAssocs(dtmr,"spoke",0.1)
#histogram
wf=data.frame(term=names(freqr),occurrences=freqr)
library(ggplot2)
p<-ggplot(subset(wf,freqr>26),aes(term,occurrences))
p<-p+geom_bar(stat="identity")
p<-p+theme(axis.text.x=element_text(angle=45,hjust=1))
p
library(wordcloud)
#setting the same seeds for consistent look
set.seed(30)
#limit words by specifying the min frequency
wordcloud(names(freqr),freqr,min.freq=27)
#..now adding color
wordcloud(names(freqr),freqr,min.freq = 27,colors=brewer.pal(6,"Dark2"))
install.packages("wordcloud2")
library(wordcloud2)
#setting the same seeds for consistent look
set.seed(30)
#limit words by specifying the min frequency
wordcloud(names(freqr),freqr,min.freq=27)
#..now adding color
wordcloud(names(freqr),freqr,min.freq = 27,colors=brewer.pal(6,"Dark2"))
library(wordcloud2)
#setting the same seeds for consistent look
set.seed(30)
#limit words by specifying the min frequency
wordcloud2(names(freqr),freqr,min.freq=27)
#..now adding color
wordcloud2(names(freqr),freqr,min.freq = 27,colors=brewer.pal(6,"Dark2"))
library(wordcloud2)
#setting the same seeds for consistent look
set.seed(30)
#limit words by specifying the min frequency
wordcloud(names(freqr),freqr,min.freq=27)
#..now adding color
wordcloud(names(freqr),freqr,min.freq = 27,colors=brewer.pal(6,"Dark2"))
source("~/Text Mine/TextMine.R")
install.packages("tm")
